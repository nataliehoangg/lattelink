# â˜• Lattelink

Find the best cafÃ©s to work or study inâ€”complete with reliable Wi-Fi, plenty of outlets, laptop-friendly seating, and a calm-but-productive vibe. Lattelink scrapes Google Places (and soon Yelp) to generate a â€œworkabilityâ€ score for each cafÃ© and presents the data through a cozy, search-first interface.

---

## ğŸš€ Features

- **Search-first homepage** with a hero split featuring blurred cafÃ© photography.
- **Weighted workability score** that prioritises outlets, seating, and reliable Wi-Fi while still showing noise levels.
- **Detailed cafÃ© profiles** including amenity breakdowns, recent reviews, tags, and direct links.
- **Dual browsing modes**: list view for quick scanning and a map view with a ranked, scrollable sidebar.
- **Translucent global navbar** with links to `HOME` and `ABOUT` (the About page mirrors the hero aesthetic and includes the project story + portfolio link).
- **Sentiment-driven insights** derived from Google Places reviews using VADER + TextBlob.
- **Scraper-driven dataset** that stays currentâ€”each city scrape replaces outdated documents and re-computes the latest scores.

---

## ğŸ—ï¸ Project Structure

```
lattelink/
â”œâ”€â”€ frontend/          # Next.js app (App Router, Tailwind, Framer Motion)
â”œâ”€â”€ backend/           # Express.js API + Mongoose models
â””â”€â”€ scraper/           # Python scraper integrating Google Places APIs
```

---

## ğŸ› ï¸ Tech Stack

| Area      | Technology                                                |
|-----------|-----------------------------------------------------------|
| Frontend  | Next.js 14, React, Tailwind CSS, Framer Motion, SWR       |
| Backend   | Node.js, Express.js, Mongoose                             |
| Database  | MongoDB / MongoDB Atlas                                   |
| Scraper   | Python 3.x, `requests`, Google Places API, TextBlob, VADER |

---

## ğŸ“¦ Setup

### Prerequisites

- Node.js 18+
- Python 3.9+
- MongoDB (local or Atlas cluster)

### 1. Install dependencies

```bash
npm run install:all
```

This bootstraps the root, frontend, and backend workspaces.

### 2. Configure environment variables

**Backend** (`backend/.env`):
```
MONGODB_URI=mongodb://localhost:27017/lattelink    # or your Atlas connection string
PORT=3001
NODE_ENV=development
```

If you use MongoDB Atlas, replace the URI with your SRV connection string (including `/lattelink`). No quotes neededâ€”URL-encode any special password characters.

**Frontend** (`frontend/.env.local`):
```
NEXT_PUBLIC_API_URL=http://localhost:3001
NEXT_PUBLIC_GOOGLE_MAPS_API_KEY=YOUR_BROWSER_KEY   # see Google Maps setup
```

**Scraper** (`scraper/.env`):
```
MONGODB_URI=mongodb://localhost:27017/lattelink     # must match the backend DB
GOOGLE_PLACES_API_KEY=YOUR_SERVER_KEY
# YELP_API_KEY=                                      # optional for future expansion
```

> All `.env` files are gitignored. Never commit keysâ€”use local `.env` files and environment variables in production.

### 3. Start the development servers

```bash
npm run dev
```

By default this launches:

- Backend at `http://localhost:3001`
- Frontend at `http://localhost:3000` (Next.js will bump the port if 3000 is busy; check the console output)

---

## ğŸ”‘ Google Maps & Places Keys

1. Follow [GOOGLE_MAPS_SETUP.md](./GOOGLE_MAPS_SETUP.md) for a step-by-step walkthrough.
2. Enable **Maps JavaScript API** (for the frontend) and **Places API** (for the scraper).
3. Create two keys:
   - **Frontend key** â€“ restrict by HTTP referrer (e.g. `http://localhost:3000/*`).  
   - **Scraper key** â€“ restrict by IP address and API (Places). If your ISP rotates IPs frequently, you can temporarily remove the IP restriction while testing.

> If the map fails to load, the UI now shows an overlay with guidance (invalid key, billing disabled, etc.). Check the browser console for the exact Google error message.

---

## ğŸ§  How Workability Scores Are Calculated

1. **Review analysis** â€“ Every Google review is processed with VADER + TextBlob. Keywords for Wi-Fi, outlets, seating, and noise are extracted to produce sentiment scores in the `[-1, 1]` range.
2. **Amenity scoring** â€“ Sentiment averages are scaled to `0â€“10`. Outlet availability is only marked â€œavailableâ€ if at least two reviews positively mention outlets.
3. **Weighted rating** â€“ Final score (0â€“10) uses the following weights:
   - Outlets: **45â€¯%** (with a heavy penalty if outlets arenâ€™t confirmed)
   - Seating comfort: **30â€¯%**
   - Wi-Fi reliability: **20â€¯%**
   - Noise level: **5â€¯%** (still displayed, but almost no impact on the ranking)
4. **Sorting** â€“ API responses default to descending `workabilityScore`.

The backend and scraper both run the same calculation so the value stays consistent regardless of the data ingest path.

---

## ğŸ”„ Running the Scraper

```bash
cd scraper
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install -r requirements.txt
python scraper.py --city "Berkeley" --max-results 40
```

Tips:

- You can scrape multiple cities by rerunning the command with different `--city` values (e.g. `"San Francisco"`, `"Oakland"`, `"Alameda"`).
- Itâ€™s technically possible to scrape cities in parallel in separate terminals, but Google Places has strict rate limits. Sequential runs are safer to avoid `OVER_QUERY_LIMIT` errors.
- Each scrape replaces the cafÃ©s (and associated reviews) for the target city, ensuring stale data is removed before new records are inserted.

---

## ğŸ“ API Endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /api/cafes?city=Berkeley` | List cafÃ©s filtered by city (defaults to workability sorting) |
| `GET /api/cafes/:id` | Fetch detailed cafÃ© profile (amenities, reviews, tags, etc.) |
| `GET /api/reviews?cafeId=<id>` | Fetch recent reviews for a specific cafÃ© |
| `POST /api/reviews` | Submit a user-supplied review (validated + linked to a cafÃ©) |
| `POST /api/reviews/:id/helpful` | Increment the â€œhelpfulâ€ counter for a review |

> The frontend currently consumes the first two endpoints. Review endpoints are ready for future user-generated feedback features.

---

## ğŸ§¹ Data Pipeline & Scraper Workflow

1. **Text Search** â€“ For each requested city we call Google Places Text Search (`cafes for working in {city}`) to fetch up to _N_ candidates.
2. **Details Enrichment** â€“ For every candidate we request Place Details to gather reviews, address components, phone, hours, and precise coordinates.
3. **Normalisation & Filtering** â€“ We convert results into a common schema, skipping anything that looks primarily like a restaurant/bar, and capture `place_id` for dedupe.
4. **Sentiment Analysis** â€“ Review text is analysed with VADER + TextBlob to produce per-factor sentiment (Wi-Fi, outlets, seating, noise).
5. **Amenity Scoring** â€“ Sentiment averages are converted to 0â€“10 scores; outlets are only marked â€œavailableâ€ when reviewers repeatedly praise them.
6. **Weighted Workability** â€“ The scraper calculates the weighted score (outletsâ€¯>â€¯seatingâ€¯>â€¯Wi-Fiâ€¯>â€¯noise) and stores amenities/tags for display.
7. **Upsert & Cleanup** â€“ Before inserting new cafÃ©s, the scraper removes existing records for that city (and associated reviews) so results stay fresh. Upserts key on `place_id`, `yelp_id`, or name+address to avoid duplicates.

> You can run multiple cities sequentially (recommended). Parallel scrapes from separate terminals work, but Google rate limiting becomes more likely.

---

## ğŸ¨ UI Notes

- Tailwind CSS powers the editorial feel (creams, espresso browns, soft grays).
- Global navbar is translucent with backdrop blur and anchors to `HOME` / `ABOUT`.
- Map view includes a ranked sidebar to keep context while exploring.
- Detail pages are padded to account for the fixed navbar so navigation stays visible.

---

## ğŸš€ Deployment (Future Considerations)

- **Frontend:** Vercel or Netlify  
- **Backend:** Render, Railway, Fly.io  
- **Database:** MongoDB Atlas  
- **Scraper:** Scheduled job on Render, Railway, AWS Lambda, or a simple cron on a VM

---

## ğŸ Troubleshooting

| Issue | What to check |
|-------|---------------|
| Map overlay says the key is invalid | Confirm `NEXT_PUBLIC_GOOGLE_MAPS_API_KEY` matches the frontend key and allows the current `localhost` origin. |
| Places API returns `REQUEST_DENIED` | IP restrictions for the scraper key likely donâ€™t include your current IPv4/IPv6, or the Places API isnâ€™t enabled. |
| Backend canâ€™t connect to MongoDB | Verify the `MONGODB_URI` works in `mongosh`; make sure the database name appears before the query string (`/lattelink?...`). |
| Scores donâ€™t reflect new weighting | Run the scraper againâ€”the new weights are applied when cafÃ©s are refreshed. |
